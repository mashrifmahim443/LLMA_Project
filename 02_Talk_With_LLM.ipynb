{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "from langchain_groq import ChatGroq\n",
    "llmaChatModel=ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000244320699D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002443206B190>, model_name='llama3-70b-8192', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmaChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for completion model\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} a story about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmModelPrompt = prompt_template.format(adjective=\"curius\", topic=\"Tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=llmaChatModel.invoke(llmModelPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s a curious story about Nikola Tesla:\\n\\n**The Time Tesla Paid an Electric Bill with a Death Ray**\\n\\nIn the early 1900s, Nikola Tesla was living in New York City, working on his most ambitious project yet: a system for transmitting electrical energy wirelessly over long distances. He had set up a laboratory in Wardenclyffe, Long Island, where he was building a massive tower to test his technology.\\n\\nOne day, the local electric company, Edison Electric Light Company (yes, founded by Thomas Edison, Tesla\\'s rival), sent Tesla a bill for the electricity he was using to power his laboratory. The bill was substantial, and Tesla, being the eccentric genius that he was, decided to respond in a rather... unconventional way.\\n\\nTesla invited the Edison Electric Light Company\\'s representatives to his laboratory, where he showed them a strange-looking contraption that resembled a large, cylindrical cannon. He claimed that this was his \"teleautomaton,\" a device capable of transmitting electrical energy wirelessly over long distances.\\n\\nTesla then proceeded to demonstrate the device\\'s power by using it to light up a row of lamps in the laboratory, without any wires or connections. The representatives were amazed, but also a bit skeptical.\\n\\nTo further impress them, Tesla offered to pay the electric bill with a demonstration of his \"death ray,\" a device that he claimed could destroy any object at a distance of several miles. He set up a target in the laboratory, a large metal plate, and proceeded to energize the death ray.\\n\\nThe representatives watched in awe as the metal plate began to glow red hot, and then suddenly, it vaporized into thin air. Tesla turned to them and said, \"I suppose this settles the bill, gentlemen?\"\\n\\nThe representatives were stunned, and they quickly departed, never to bother Tesla again about his electric bill. From that day on, Tesla\\'s laboratory was known as the \"Wardenclyffe Mystery,\" and the legend of his death ray lived on.\\n\\nWhile this story might sound like science fiction, it\\'s actually based on Tesla\\'s own accounts and those of his contemporaries. The truth behind the \"death ray\" remains a mystery, but it\\'s a fascinating example of Tesla\\'s creativity, showmanship, and innovative spirit.' response_metadata={'token_usage': {'completion_tokens': 454, 'prompt_tokens': 19, 'total_tokens': 473, 'completion_time': 1.321352426, 'prompt_time': 0.004563705, 'queue_time': 0.035151375, 'total_time': 1.325916131}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None} id='run-abee9033-54e9-4d0c-ae17-4b58156bbbd4-0' usage_metadata={'input_tokens': 19, 'output_tokens': 454, 'total_tokens': 473}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 29 grandchildren. His nine children, including John F. Kennedy, Robert F. Kennedy, and Edward M. Kennedy, went on to have large families of their own, resulting in a sizable number of grandchildren.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"The Kennedy family\",\n",
    "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
    ")\n",
    "\n",
    "response = llmaChatModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡hasta luego!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = final_prompt | llmaChatModel\n",
    "\n",
    "ct=chain.invoke(\"\")\n",
    "print(ct.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
